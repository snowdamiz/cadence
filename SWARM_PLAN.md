# Swarm Plan: Multi-Agent Parallel Execution for Cadence

## 1. Problem Statement

Cadence currently operates as a single-agent, strictly sequential workflow. The routing engine (`workflow_state.py`) finds the first non-complete actionable task and makes it the singular `next_item`. There is no concept of parallel work, agent identity, or distributed task execution.

This plan introduces **swarm execution** as a first-class concept in Cadence: multiple AI agent instances working on a project simultaneously, orchestrated through GitHub's native versioning primitives (branches, PRs, issues) and Cadence's existing state machine. Swarm is not a mode toggle or opt-in feature -- it is the core execution model. A single agent working alone is simply a swarm of one.

### What This Plan Does NOT Do

- Does not require agents to have separate GitHub accounts
- Does not introduce a central server, message queue, or external coordination service
- Does not require multiple machines (agents on a single machine are fully supported via git worktrees)
- Does not change the Foundation phase (scaffold, prereqs, intake, ideation/brownfield, research remain sequential and single-agent)

---

## 2. Design Principles

1. **GitHub-native coordination.** Branches, PRs, and Issues are the coordination fabric. No proprietary lock servers or databases.
2. **Account-agnostic identity.** Agent identity lives in commit trailers and branch names, not GitHub user accounts. All agents may share a single GitHub account.
3. **Ownership zones prevent conflicts.** Tasks declare which files/directories they may touch. Tasks within a wave have non-overlapping zones. This is the primary merge-conflict prevention mechanism.
4. **Coordinator + Worker separation.** One coordinator role manages the plan and work queue. Worker agents only implement. This eliminates state contention on `cadence.json`.
5. **Fail-safe over fail-fast.** Agents can die mid-task without corrupting shared state. Recovery is always possible from GitHub's durable state (issues, PRs, branches).
6. **Planning thinks long.** The planner produces a full multi-milestone roadmap upfront, then decomposes the active milestone into parallel-safe work units. This avoids the "one milestone at a time, stuff everything in" anti-pattern.

---

## 3. Agent Identity Model

### 3.1 Identity Generation

Each agent instance generates a unique identity at swarm-join time:

```
Format: <role>-<short-hash>
Examples: worker-a7b3, worker-c1d4, coordinator-f2e8
```

- `<role>` is `worker` or `coordinator`
- `<short-hash>` is 4 hex chars from a random UUID
- Identity is ephemeral per-session but stable for the session's lifetime
- Generated by a new `swarm-join.py` script and persisted in `.cadence/agents/<agent-id>.json`

### 3.2 Identity in Git

Agent identity appears in three places, none of which require a separate GitHub account:

**Commit trailers** (structured, searchable, don't pollute subject line):
```
cadence(executor): implement user authentication [src]

Cadence-Agent: worker-a7b3
Cadence-Task: task-impl-user-auth
Cadence-Wave: wave-backend-core-1
```

**Branch names** (instantly identifies agent and task):
```
cadence/worker-a7b3/task-impl-user-auth
```

**Issue/PR comments** (durable coordination record):
```
<!-- cadence-agent: worker-a7b3 -->
<!-- cadence-claimed-at: 2026-02-23T15:30:00Z -->
Claimed by agent worker-a7b3.
```

### 3.3 Identity Discovery

Any observer (human or agent) can reconstruct who did what by:
- `git log --grep="Cadence-Agent: worker-a7b3"` for all commits by an agent
- `git branch --list "cadence/worker-a7b3/*"` for all branches
- GitHub Issue search by label `cadence-agent:worker-a7b3`
- No GitHub account mapping required anywhere

---

## 4. State Architecture

### 4.1 The Core Problem

Multiple agents writing to `.cadence/cadence.json` simultaneously guarantees conflicts. The current design assumes a single writer.

### 4.2 Solution: Split State + GitHub as Source of Truth

State is split across three tiers:

```
Tier 1: Plan Structure (main branch cadence.json)
  - The milestone/phase/wave/task hierarchy
  - Written ONLY by the coordinator
  - Workers never modify this directly

Tier 2: Execution State (GitHub Issues + PR metadata)
  - Which tasks are claimed, in-progress, done
  - Tracked via issue labels, comments, and PR status
  - This is the distributed coordination layer
  - No file contention because it's GitHub API, not files

Tier 3: Agent-Local State (.cadence/agents/<agent-id>.json)
  - Per-agent working context on their feature branch
  - Tracks the agent's current task, progress, local decisions
  - Never merges into main (lives only on agent branches)
  - Deleted after PR is merged
```

### 4.3 State File Changes

**Main branch `cadence.json` additions:**

```json
{
  "swarm": {
    "branch_prefix": "cadence",
    "issue_label_namespace": "cadence",
    "coordinator_agent_id": "",
    "max_concurrent_agents": 0,
    "wave_advancement": "auto|manual"
  },
  "workflow": {
    "plan": [
      {
        "id": "milestone-mvp",
        "kind": "milestone",
        "title": "MVP",
        "intent": "Deliver core user-facing functionality",
        "success_criteria": [
          "Users can sign up, log in, and perform core actions",
          "API responds under 200ms p95",
          "80% test coverage on critical paths"
        ],
        "children": [
          {
            "id": "phase-core-infra",
            "kind": "phase",
            "title": "Core Infrastructure",
            "children": [
              {
                "id": "wave-backend-core-1",
                "kind": "wave",
                "title": "Backend Foundations",
                "parallel": true,
                "children": [
                  {
                    "id": "task-impl-auth",
                    "kind": "task",
                    "title": "Implement authentication system",
                    "scope_description": "JWT-based auth with refresh tokens...",
                    "acceptance_criteria": [
                      "Login/register endpoints return valid JWTs",
                      "Refresh token rotation works",
                      "Unit tests pass"
                    ],
                    "ownership_zones": [
                      "src/auth/**",
                      "tests/auth/**",
                      "src/middleware/auth.ts"
                    ],
                    "complexity": "M",
                    "assigned_agent": "",
                    "issue_number": null,
                    "pr_number": null,
                    "branch": "",
                    "route": {
                      "skill_name": "executor",
                      "skill_path": "skills/executor/SKILL.md",
                      "reason": "Task implementation not started."
                    }
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "id": "milestone-beta",
        "kind": "milestone",
        "title": "Beta",
        "intent": "Stability, performance, and beta user onboarding",
        "success_criteria": ["..."],
        "children": []
      },
      {
        "id": "milestone-ga",
        "kind": "milestone",
        "title": "General Availability",
        "intent": "Production hardening and public launch",
        "success_criteria": ["..."],
        "children": []
      }
    ]
  }
}
```

**Agent-local state file (`.cadence/agents/<agent-id>.json`):**

```json
{
  "agent_id": "worker-a7b3",
  "role": "worker",
  "created_at": "2026-02-23T15:30:00Z",
  "current_task": {
    "task_id": "task-impl-auth",
    "issue_number": 42,
    "branch": "cadence/worker-a7b3/task-impl-auth",
    "claimed_at": "2026-02-23T15:31:00Z",
    "status": "in_progress"
  },
  "session_history": []
}
```

### 4.4 Why GitHub Issues as Coordination Layer

Issues are chosen over file-based state for distributed coordination because:

1. **Atomic operations.** Adding a label, posting a comment, or closing an issue is atomic via the GitHub API. No file-level merge conflicts.
2. **Visibility.** Humans can see the work queue in the GitHub Issues tab without any Cadence tooling.
3. **Durability.** If all agents crash, the issue state is preserved on GitHub. Recovery means reading issue state, not reconstructing from git history.
4. **Searchability.** Labels, milestones, and search make it easy to query "what's in progress" or "what's available for wave 2."
5. **Native to GitHub.** No additional infrastructure required. Works with `gh` CLI that Cadence already supports.

---

## 5. Planning Model: Milestones-First Architecture

### 5.1 The Anti-Pattern This Avoids

The GSD (get-shit-done) approach plans one milestone at a time and stuffs everything into it without considering the broader project arc. This leads to:
- Milestones that are actually just batches of unrelated work
- No visibility into what comes after the current milestone
- Scope creep because there's no future plan to push things to
- No ability to parallelize because work isn't decomposed for it

### 5.2 The Cadence Planning Approach

Planning happens in two stages:

**Stage 1: Roadmap Planning (all milestones)**

The planner reads ideation + research findings and produces:
- A complete milestone sequence for the entire project
- Each milestone has a clear intent, scope boundary, and success criteria
- Milestones represent meaningful user-visible deliverables (MVP, Beta, GA, etc.)
- Later milestones have lighter detail (intent + success criteria only)
- The active milestone gets full decomposition

```
Project Roadmap
├── Milestone 1: MVP              ← FULLY PLANNED (phases, waves, tasks)
│   ├── Phase 1: Core Infrastructure
│   ├── Phase 2: User Features
│   ├── Phase 3: Integration & Testing
│   └── Phase 4: Polish & Deploy
├── Milestone 2: Beta             ← SCOPED (intent + success criteria only)
│   └── [detailed planning happens when M1 completes]
├── Milestone 3: GA               ← SCOPED
│   └── [detailed planning happens when M2 completes]
└── Milestone 4: Post-Launch      ← SCOPED
    └── [detailed planning happens when M3 completes]
```

**Stage 2: Active Milestone Decomposition**

For the active milestone, the planner produces:
- Phases that represent logical groupings of related work
- Waves within phases that define parallelism boundaries
- Tasks within waves that are implementation-ready work units

### 5.3 Wave Design: The Parallelism Boundary

A wave is the unit of parallel execution. Within a wave:

- All tasks can execute simultaneously (no inter-task dependencies)
- Each task has non-overlapping ownership zones
- Tasks are sized to be completable in a single agent session
- The wave is complete only when ALL tasks are merged to main

Between waves:

- Wave N+1 starts only after wave N is fully merged
- This creates natural integration points where all parallel work converges
- Tests run after each wave merge to catch integration issues early

```
Phase 1: Core Infrastructure
├── Wave 1 (parallel): 3 agents can work simultaneously
│   ├── task-impl-auth       → owns src/auth/**, tests/auth/**
│   ├── task-impl-database   → owns src/db/**, tests/db/**, migrations/**
│   └── task-impl-config     → owns src/config/**, tests/config/**
│
├── Wave 2 (parallel): depends on wave 1 being fully merged
│   ├── task-impl-api-users  → owns src/api/users/**, tests/api/users/**
│   └── task-impl-api-core   → owns src/api/core/**, tests/api/core/**
│
└── Wave 3 (sequential): depends on wave 2
    └── task-integration-tests → owns tests/integration/**
```

### 5.4 Ownership Zones

Ownership zones are glob patterns declaring which files a task may create or modify. They are the primary mechanism preventing merge conflicts between parallel agents.

**Rules:**
1. Within a wave, no two tasks may have overlapping ownership zones
2. The planner validates non-overlap at plan creation time
3. Shared files (e.g., `package.json`, `src/index.ts`) must be assigned to exactly one task per wave, or deferred to a dedicated integration task
4. Ownership zones are advisory for the agent but enforced at PR review time
5. A new validation script (`validate-ownership-zones.py`) checks zones at plan creation and before PR merge

**Handling shared files:**
Some files inevitably need changes from multiple tasks (e.g., a root barrel export, a router registration file). The planner should:
- Assign shared file ownership to a single "integration" task at the end of the wave
- Or structure the code so that each task creates its own file and a subsequent wave task wires them together
- Document which shared files exist and why the ownership assignment was made

### 5.5 Task Sizing

Tasks should be scoped so a single agent can complete one in a single session. The planner assigns complexity estimates:

| Complexity | Scope | Typical Files | Agent Sessions |
|-----------|-------|---------------|----------------|
| S | Single concern, few files | 1-5 | 1 |
| M | Feature with tests | 5-15 | 1-2 |
| L | Multi-component feature | 15-30 | 2-3 |
| XL | Architectural change | 30+ | 3+ (should be split) |

XL tasks should be split into multiple M/L tasks. If a task can't be split, it becomes a sequential wave with a single task.

### 5.6 Dependency Model

**Implicit dependencies** (wave ordering): Tasks in wave N+1 depend on all tasks in wave N being complete. No explicit declaration needed.

**Explicit cross-phase dependencies** (rare, discouraged): If a task in Phase 2 depends on a specific task in Phase 1 (not the whole phase), this is declared as:

```json
{
  "id": "task-impl-notifications",
  "depends_on": ["task-impl-auth"],
  "depends_on_reason": "Needs auth middleware for notification permissions"
}
```

The coordinator validates explicit dependencies before issuing work.

---

## 6. New Skills and Scripts

### 6.1 Planner Skill (`skills/planner/SKILL.md`)

**Purpose:** Transform ideation + research output into a multi-milestone roadmap with parallel-safe task decomposition.

**Inputs:**
- `ideation` object from cadence.json (objective, scope, constraints, research findings)
- `project-details` (greenfield/brownfield mode, baseline inventory)

**Outputs:**
- Complete `workflow.plan` with multiple milestones
- Active milestone fully decomposed into phases/waves/tasks
- Each task has: scope description, acceptance criteria, ownership zones, complexity estimate
- Ownership zone validation passes (no overlaps within waves)

**Behavior:**
1. Run entry gate: `run-skill-entry-gate.py --require-cadence --assert-skill-name planner`
2. Read ideation and research: `get-ideation.py --project-root "$PROJECT_ROOT"`
3. Present roadmap overview to user for validation:
   - List all planned milestones with intent and success criteria
   - Ask user to confirm or adjust milestone scope/ordering
4. For the active milestone, present detailed decomposition:
   - Show phases and their rationale
   - Show waves within each phase, highlighting parallelism opportunities
   - Show tasks with ownership zones and how they were derived
5. Validate ownership zones: `validate-ownership-zones.py --project-root "$PROJECT_ROOT"`
6. Ask user to confirm or adjust the plan
7. Persist plan: `inject-plan.py --project-root "$PROJECT_ROOT" --stdin`
8. Finalize: `finalize-skill-checkpoint.py --scope planner --checkpoint plan-created`
9. End with: `Start a new chat and say "start the swarm".` (even for a single agent -- a swarm of one uses the same flow)

**Key design decisions the planner must make:**
- How many milestones to plan (based on project scope)
- How to group work into phases (logical cohesion)
- How to partition work into waves (maximum parallelism while maintaining independence)
- How to size tasks (completable in a single agent session)
- How to assign ownership zones (no overlaps, handle shared files)

### 6.2 Swarm Coordinator Skill (`skills/swarm-coordinator/SKILL.md`)

**Purpose:** Manage the swarm lifecycle: issue work, track progress, merge results, advance waves.

**This skill is invoked by the coordinator agent (human or AI).**

**Behavior:**
1. Run entry gate with coordinator identity
2. Read current plan and wave status
3. Determine available work:
   - Current wave's unclaimed tasks → create GitHub Issues
   - In-progress tasks → check PR status
   - Completed wave → advance to next wave after validation
4. Issue management:
   - Create issues with Cadence metadata labels
   - Close issues when PRs are merged
   - Reassign stale tasks (claimed but no PR within timeout)
5. PR management:
   - Validate PR against ownership zones before merge approval
   - Run tests after merge
   - Update cadence.json task status after merge
6. Wave advancement:
   - When all tasks in a wave have merged PRs:
     - Run integration tests
     - If tests pass, mark wave complete
     - Create issues for the next wave's tasks
   - If tests fail, create a fix task in the current wave

### 6.3 Swarm Worker Skill (`skills/swarm-worker/SKILL.md`)

**Purpose:** Claim a task, implement it on a feature branch, and open a PR.

**This skill is invoked by each worker agent instance.**

**Behavior:**
1. Generate or resume agent identity: `swarm-join.py --project-root "$PROJECT_ROOT"`
2. Run entry gate
3. Find available work:
   - Query GitHub Issues: `gh issue list --label "cadence-task,cadence-available" --json number,title,labels,body`
   - If no available work, report and stop
4. Claim a task (branch-first protocol -- see Section 8.2):
   - Create feature branch: `git branch cadence/<agent-id>/<task-id> main`
   - Create worktree: `git worktree add .cadence/worktrees/<agent-id>--<task-id> <branch>`
   - If worktree creation fails (branch already checked out by another agent), skip this task and try the next available one
   - Only after worktree succeeds: swap issue labels (`cadence-available` → `cadence-claimed`) and post agent comment
   - If the label swap fails (already claimed by a remote agent), remove the worktree and try the next task
5. Implement the task:
   - Read task details from issue body (scope, acceptance criteria, ownership zones)
   - Work within declared ownership zones
   - Follow existing code conventions
   - Write tests as specified in acceptance criteria
6. Validate work:
   - Ensure all changed files are within ownership zones
   - Run relevant tests
7. Open PR:
   - Title: `cadence(executor): <task-title>`
   - Body: Cadence metadata (task ID, wave, agent ID, ownership zones, acceptance criteria checklist)
   - Reference: `Closes #<issue-number>`
   - Commits include `Cadence-Agent: <agent-id>` trailer
8. Stop and report PR URL

### 6.4 Executor Logic (Internal to Swarm Worker)

The swarm-worker skill embeds the executor logic directly. There is no separate executor skill -- the worker IS the executor:

1. Read task details from the claimed issue (scope, acceptance criteria, ownership zones)
2. Analyze the codebase within ownership zones
3. Implement changes
4. Write tests
5. Validate changes are within ownership zones
6. Checkpoint commits with `cadence(executor)` scope and `Cadence-Agent` trailer

### 6.5 New Scripts

| Script | Purpose |
|--------|---------|
| `swarm-join.py` | Generate agent identity, create agent-local state file |
| `swarm-status.py` | Read swarm state from GitHub Issues/PRs, report available work |
| `validate-ownership-zones.py` | Check no ownership zone overlaps within waves |
| `inject-plan.py` | Validate and persist the planner's multi-milestone plan |
| `create-task-issues.py` | Create GitHub Issues for a wave's tasks with Cadence metadata |
| `claim-task.py` | Branch-first task claim: create worktree (atomic lock), then label-swap the issue. Rolls back on conflict. |
| `reconcile-wave-status.py` | Check PR merge status for all tasks in a wave, determine if wave is complete |
| `advance-wave.py` | Mark current wave complete, prepare next wave |
| `validate-pr-zones.py` | Check PR changed files against task's declared ownership zones |

---

## 7. Workflow Lifecycle (End to End)

### 7.1 Updated Task Sequence

```
Foundation (existing, single-agent, unchanged):
  task-scaffold → scaffold
  task-prerequisite-gate → prerequisite-gate
  task-brownfield-intake → brownfield-intake
  task-brownfield-documentation → brownfield-documenter  (brownfield only)
  task-ideation → ideator                                 (greenfield only)
  task-research → researcher

Planning (new, single-agent):
  task-roadmap-planning → planner

Execution (new, always swarm -- single-agent is a swarm of one):
  task-execute-<id> → swarm-worker  (agents claim tasks from the wave)
```

### 7.2 Swarm Execution Flow

```
                    ┌─────────────────────────┐
                    │  Coordinator reads plan  │
                    │  from main branch        │
                    └──────────┬──────────────┘
                               │
                    ┌──────────▼──────────────┐
                    │  Create GitHub Issues    │
                    │  for Wave 1 tasks        │
                    │  (label: cadence-available)│
                    └──────────┬──────────────┘
                               │
              ┌────────────────┼────────────────┐
              │                │                │
    ┌─────────▼─────┐ ┌───────▼───────┐ ┌──────▼──────┐
    │  Worker Alpha  │ │ Worker Bravo  │ │ Worker Gamma│
    │  Claims task A │ │ Claims task B │ │ Claims task C│
    │  Branch: ...   │ │ Branch: ...   │ │ Branch: ... │
    │  Implements    │ │ Implements    │ │ Implements  │
    │  Opens PR      │ │ Opens PR      │ │ Opens PR    │
    └───────┬───────┘ └───────┬───────┘ └──────┬──────┘
            │                 │                │
            └────────────────┬┘────────────────┘
                             │
                  ┌──────────▼──────────────┐
                  │  Coordinator reviews     │
                  │  Validates ownership     │
                  │  Merges PRs to main      │
                  │  Runs integration tests  │
                  └──────────┬──────────────┘
                             │
                  ┌──────────▼──────────────┐
                  │  Wave 1 complete?        │
                  │  Yes → Advance to Wave 2 │
                  │  No  → Wait for PRs      │
                  └─────────────────────────┘
```

### 7.3 Agent Session Lifecycle

Each worker agent session follows this sequence:

```
1. Start new chat with agent
2. "Join the swarm" or "start working"
   → swarm-join.py generates/resumes agent identity
   → swarm-status.py finds available tasks
3. Agent claims a task
   → claim-task.py labels issue, creates branch
4. Agent implements
   → Works within ownership zones
   → Makes commits with Cadence-Agent trailer
5. Agent opens PR
   → PR references issue, includes Cadence metadata
6. Agent stops
   → "Task complete. PR #<n> opened for review."
```

A coordinator session:

```
1. Start new chat with agent (or human reviews)
2. "Check swarm status"
   → reconcile-wave-status.py checks all PR statuses
3. Coordinator reviews/merges PRs
   → validate-pr-zones.py checks ownership compliance
   → Merge PR
   → Update cadence.json task status
4. If wave complete:
   → advance-wave.py marks wave done
   → create-task-issues.py issues next wave's tasks
5. Coordinator stops
```

---

## 8. Branching Strategy

### 8.1 Branch Naming Convention

```
cadence/<agent-id>/<task-id>

Examples:
  cadence/worker-a7b3/task-impl-auth
  cadence/worker-c1d4/task-impl-database
  cadence/coordinator-f2e8/plan-update
```

### 8.2 Same-Machine Isolation via Git Worktrees

**The problem:** Git's working tree, index, and checked-out branch are per-clone state. Two agents sharing a single clone would clobber each other's branch checkouts, staged files, and uncommitted work.

**The solution:** `git worktree`. This is a native git feature (not a third-party tool) that creates additional working directories linked to the same repository. Each worktree has its own checked-out branch, index, and working directory, but they share the same `.git` history, remotes, and reflog.

```
project/                              ← main clone (coordinator works here)
  .git/
  .cadence/cadence.json
  src/...

project/.cadence/worktrees/
  worker-a7b3--task-impl-auth/        ← worktree for agent Alpha
    src/...                           (checked out: cadence/worker-a7b3/task-impl-auth)
  worker-c1d4--task-impl-database/    ← worktree for agent Bravo
    src/...                           (checked out: cadence/worker-c1d4/task-impl-database)
```

**Worktree lifecycle (managed by `claim-task.py`):**

1. Agent selects a task from the available issue list
2. Script creates branch: `git branch cadence/<agent-id>/<task-id> main`
3. Script creates worktree: `git worktree add .cadence/worktrees/<agent-id>--<task-id> cadence/<agent-id>/<task-id>`
4. If step 3 fails (branch already checked out by another worktree), the claim is lost -- skip to the next available task
5. If step 3 succeeds, the worktree acts as the **local lock**. Script then label-swaps the GitHub Issue (`cadence-available` → `cadence-claimed`) and posts the agent comment
6. If the label swap reveals a remote conflict (another agent on a different machine claimed the same issue first), remove the worktree (`git worktree remove`) and delete the branch, then try the next task
7. Agent's `PROJECT_ROOT` is set to the worktree path for the duration of the task
8. Agent works entirely within its worktree -- no interference with other agents or the main checkout
9. Agent commits and pushes from its worktree
10. After PR merge, script cleans up: `git worktree remove .cadence/worktrees/<agent-id>--<task-id>` and `git branch -d <branch>`

**Key properties:**
- **Full isolation:** Each agent has its own working directory, branch, and index. No races on `git add`, `git commit`, or branch checkout.
- **Shared history:** All worktrees see the same commits, remotes, and tags. `git fetch` in any worktree updates all of them.
- **Disk efficient:** Worktrees share the object store (`.git/objects`). Only the working tree files are duplicated.
- **No special tooling:** `git worktree` has been stable since Git 2.5 (2015). Every machine with git has it.
- **Single machine, many agents:** Three CLI agent windows on the same laptop can each work in their own worktree without conflict.
- **Claim authority:** The worktree is the local lock (same-machine conflict resolution) and the issue label is the distributed lock (cross-machine conflict resolution). Both must succeed for a claim to hold.

**Git constraint:** A branch can only be checked out in one worktree at a time. This is enforced by git itself and provides a natural same-machine mutex -- if two agents try to claim the same task, the second `git worktree add` will fail because the branch is already checked out.

**Shared lockfile contention:** Worktrees share `.git/objects` and `.git/refs`. Concurrent git operations (e.g., two agents running `git fetch` or `git push` simultaneously) can transiently fail with "Unable to create lock file" errors. All git-calling scripts (`claim-task.py`, `git-checkpoint.py`, etc.) must retry git operations on lockfile errors with exponential backoff (3 attempts, 500ms/1s/2s delays).

### 8.3 Branch Lifecycle

```
main ─────────────────────────────────────────────────
       \                                    ↑
        \── cadence/worker-a7b3/task-auth ──┘ (PR merged)
       \                                    ↑
        \── cadence/worker-c1d4/task-db ────┘ (PR merged)
```

1. Worker creates branch + worktree at claim time
2. Worker makes commits in its worktree
3. Worker opens PR from branch to `main`
4. If main has advanced (other PRs merged), worker rebases before merge
5. PR is merged (squash or merge commit, configurable)
6. Worktree is removed, branch is deleted after merge

### 8.4 Rebase Strategy for Parallel Work

When multiple agents work in parallel, earlier-merged PRs may change files that later PRs don't conflict with but need to incorporate:

1. After each PR merge to main, remaining open PRs check for rebase necessity
2. If the PR's ownership zones don't overlap with merged changes: no rebase needed
3. If shared infrastructure files changed: rebase required
4. The coordinator can trigger rebases via comments or the agent can detect staleness

### 8.5 Merge Strategy

Configurable per project. Options:
- **Squash merge** (default): Clean history, one commit per task on main
- **Merge commit**: Preserves full branch history, useful for auditing

Regardless of merge strategy, the squash/merge commit includes:
```
cadence(executor): implement user authentication

Cadence-Task: task-impl-auth
Cadence-Wave: wave-backend-core-1
Cadence-Agent: worker-a7b3
Cadence-PR: #42
```

---

## 9. GitHub Issue Schema

### 9.1 Issue Template

Each task becomes a GitHub Issue with structured metadata:

**Title:** `[Cadence] <task-title>`

**Labels:**
- `cadence-task` (all Cadence work items)
- `cadence-milestone:<milestone-id>` (e.g., `cadence-milestone:mvp`)
- `cadence-phase:<phase-id>` (e.g., `cadence-phase:core-infra`)
- `cadence-wave:<wave-id>` (e.g., `cadence-wave:backend-core-1`)
- `cadence-complexity:<S|M|L|XL>`
- `cadence-available` (unclaimed, ready for work)
- Status labels (mutually exclusive): `cadence-available` | `cadence-claimed` | `cadence-in-progress` | `cadence-in-review`

**Body:**

```markdown
## Task: <task-title>

### Scope
<scope_description from plan>

### Acceptance Criteria
- [ ] <criterion 1>
- [ ] <criterion 2>
- [ ] <criterion 3>

### Ownership Zones
Files/directories this task may modify:
- `src/auth/**`
- `tests/auth/**`
- `src/middleware/auth.ts`

### Dependencies
- Depends on: <none | list of task-ids>
- Blocks: <none | list of task-ids>

### Metadata
<!-- cadence-metadata
task_id: task-impl-auth
milestone_id: milestone-mvp
phase_id: phase-core-infra
wave_id: wave-backend-core-1
complexity: M
-->
```

### 9.2 Issue Lifecycle

```
Created (cadence-available)
  → Claimed by agent (cadence-claimed, agent comment added)
    → In progress (cadence-in-progress, branch created)
      → PR opened (cadence-in-review, PR linked)
        → PR merged (issue closed, labels cleaned up)
```

---

## 10. Commit Convention Changes

### 10.1 New Scope: `executor`

Add to `commit-conventions.json`:

```json
{
  "executor": {
    "description": "Task implementation in execution phase",
    "checkpoints": {
      "task-progress": "persist implementation progress",
      "task-complete": "complete task implementation"
    }
  },
  "planner": {
    "description": "Project roadmap and task planning",
    "checkpoints": {
      "plan-created": "persist project roadmap and task plan",
      "plan-updated": "update project plan"
    }
  },
  "swarm-coordinator": {
    "description": "Swarm coordination and wave management",
    "checkpoints": {
      "wave-issued": "issue wave tasks",
      "wave-advanced": "advance to next wave",
      "wave-reconciled": "reconcile wave completion status"
    }
  }
}
```

### 10.2 Commit Trailer Support

`git-checkpoint.py` needs a new `--agent-id` flag that appends a `Cadence-Agent` trailer to commit messages:

```
cadence(executor): implement user authentication [src]

Cadence-Agent: worker-a7b3
```

### 10.3 Atomic Group: `swarm-state`

Add a new semantic file group for agent-local state:

```json
{
  "swarm-state": {
    "tag": "swarm",
    "label": "swarm agent state",
    "patterns": [".cadence/agents/**"]
  }
}
```

---

## 11. Workflow State Engine Changes

### 11.1 Parallel-Aware Routing

Currently, `_derive_workflow` picks the single first non-complete task as `next_item`. For swarm mode, this needs to change:

**New computed fields on workflow:**
```json
{
  "next_items": [
    { "id": "task-impl-auth", ... },
    { "id": "task-impl-database", ... },
    { "id": "task-impl-config", ... }
  ],
  "active_wave": {
    "id": "wave-backend-core-1",
    "title": "Backend Foundations",
    "total_tasks": 3,
    "completed_tasks": 0,
    "in_progress_tasks": 1,
    "available_tasks": 2
  },
  "swarm_summary": {
    "active_agents": 2,
    "available_tasks": 1,
    "in_progress_tasks": 2,
    "waves_remaining": 3
  }
}
```

**Routing changes:**
- `next_items` (plural) always returns ALL unclaimed tasks in the current wave. When only one agent is running, it simply claims one task at a time.
- `next_item` (singular, legacy) continues to work by returning the first entry from `next_items`
- A task is "available" if: status is `pending`, no `assigned_agent`, all dependencies met
- Wave advancement: only progress to next wave when ALL tasks in current wave are `complete`

### 11.2 Wave Completion Logic

```python
def is_wave_complete(wave: dict) -> bool:
    """A wave is complete when all its tasks are complete or skipped."""
    children = wave.get("children", [])
    return all(
        _is_complete_status(child.get("status", "pending"))
        for child in children
    )

def get_active_wave(plan: list) -> dict | None:
    """Find the first wave that isn't complete."""
    for milestone in plan:
        for phase in milestone.get("children", []):
            for wave in phase.get("children", []):
                if not is_wave_complete(wave):
                    return wave
    return None

def get_available_tasks(wave: dict) -> list:
    """Get all unclaimed, unblocked tasks in a wave."""
    return [
        task for task in wave.get("children", [])
        if task.get("status") == "pending"
        and not task.get("assigned_agent")
    ]
```

### 11.3 Backward Compatibility

All changes are additive:
- `next_item` (singular) continues to work, returning the first entry from `next_items`
- `next_items` (plural) is always populated when execution tasks exist
- Pre-swarm cadence.json files are handled gracefully (missing `swarm` block → safe defaults)
- Existing Foundation routing (scaffold, prereqs, ideation, research) is unchanged

---

## 12. Failure Modes and Recovery

### 12.1 Agent Dies Mid-Task

**Symptoms:** Branch exists, issue is `cadence-claimed`, no PR, no recent commits.

**Recovery:**
- Coordinator detects stale claims (no commit activity for configurable timeout)
- Coordinator removes `cadence-claimed` label, adds `cadence-available`
- Adds comment: "Agent worker-a7b3 appears stale. Task unclaimed for reassignment."
- Another agent can claim and either continue from the branch or start fresh

### 12.2 PR Has Merge Conflicts

**Symptoms:** PR can't be merged due to conflicts with other merged PRs.

**Recovery:**
- Coordinator comments on PR: "Rebase required due to conflicts with #<merged-PR>"
- Agent (or human) rebases the branch
- If ownership zones were respected, conflicts should be rare and limited to shared files
- If conflicts are in ownership zones, the plan may need adjustment

### 12.3 Coordinator Failure

**Symptoms:** No wave advancement, issues not created.

**Recovery:**
- A new coordinator session can fully reconstruct state from:
  - `cadence.json` on main (plan structure)
  - GitHub Issues (current task status)
  - GitHub PRs (merge status)
- `reconcile-wave-status.py` handles this reconstruction
- No single-point-of-failure data

### 12.4 Ownership Zone Violation

**Symptoms:** PR modifies files outside its declared zones.

**Detection:**
- `validate-pr-zones.py` checks PR diff against task's ownership zones
- Runs automatically at PR review time (can be a GitHub Action)

**Response:**
- PR is flagged, not merged
- Agent must either:
  - Remove out-of-zone changes
  - Or coordinator adjusts the plan's ownership zones (if the violation reveals a planning gap)

### 12.5 Test Failures After Wave Merge

**Symptoms:** Integration tests fail after all wave PRs are merged.

**Recovery:**
- Coordinator creates a new fix task in the current wave
- Fix task gets a new issue and can be claimed by any agent
- Wave doesn't advance until fix task is also merged and tests pass

### 12.6 Task Claim Race Condition

**Symptoms:** Two agents (same machine or different machines) attempt to claim the same task simultaneously.

**Same-machine race (resolved by git worktree lock):**
- First agent's `git worktree add` succeeds (git creates a lockfile for the branch)
- Second agent's `git worktree add` fails immediately ("branch is already checked out")
- Second agent skips to the next available task -- no dirty state, no rollback needed

**Cross-machine race (resolved by issue label check):**
- Both agents create worktrees successfully (different machines, no shared git lockfile)
- Both attempt to swap the issue label from `cadence-available` to `cadence-claimed`
- `claim-task.py` reads the issue labels AFTER posting its claim comment
- If `cadence-available` was already removed by the other agent, the late claimer detects the conflict
- Late claimer removes its worktree and branch, then tries the next task
- If both succeed in the label swap (extremely narrow race), the coordinator detects the double-claim during reconciliation and reassigns one

**Prevention:**
- `claim-task.py` uses a check-after-write pattern: post comment, swap label, re-read issue, verify this agent's comment was the first `cadence-claimed` comment
- If another agent's claim comment has an earlier timestamp, yield and roll back

### 12.7 Git Lockfile Contention

**Symptoms:** Git commands fail with "Unable to create lock file" or "Another git process seems to be running" errors.

**Cause:** Multiple worktrees share `.git/objects` and `.git/refs`. Concurrent operations (`git fetch`, `git push`, `git gc`) contend on internal lockfiles.

**Recovery:**
- All Cadence scripts retry git operations on lockfile errors (3 attempts, exponential backoff: 500ms, 1s, 2s)
- If all retries fail, surface the error to the agent and let it retry the full operation later
- This is transient by nature -- lockfiles are held for milliseconds during normal operations

### 12.8 GitHub API Rate Limiting

**Symptoms:** `gh` CLI calls return HTTP 403 with "rate limit exceeded" errors.

**Cause:** Multiple agents sharing one GitHub auth token hit the same rate limit bucket (5,000 requests/hour for authenticated users).

**Mitigation:**
- `claim-task.py` and `reconcile-wave-status.py` batch API calls where possible (e.g., fetch all issues in one query, not one per task)
- Scripts check `X-RateLimit-Remaining` header and pause if approaching the limit
- `max_concurrent_agents` configuration provides an operational lever to limit API pressure
- If rate-limited, agents wait and retry with the `Retry-After` header value

### 12.9 Plan Needs Adjustment Mid-Execution

**Symptoms:** During execution, an agent discovers the plan is wrong (missing task, wrong ownership zones, etc.)

**Recovery:**
- Agent reports the issue in the task's GitHub Issue comments
- Coordinator pauses wave advancement
- Coordinator runs `planner` in update mode to adjust the plan
- New/modified tasks get new issues
- Agents continue with updated plan

---

## 13. Implementation Sequence

This feature should be implemented in this order to minimize risk:

### Phase 1: Planning Skill (no swarm yet)

**Goal:** Replace the current single-wave Foundation plan with a multi-milestone planning capability.

1. Create `inject-plan.py` script for persisting planner output
2. Create `validate-ownership-zones.py` for zone validation
3. Create `skills/planner/SKILL.md` with roadmap + decomposition behavior
4. Add `task-roadmap-planning` to the workflow after `task-research`
5. Add `planner` scope/checkpoint to `commit-conventions.json`
6. Update `workflow_state.py` to handle dynamic plan extension (tasks beyond Foundation)
7. Write tests for plan validation and ownership zone checking

### Phase 2: Single-Agent Executor

**Goal:** Add the ability to actually implement work, one task at a time.

1. Create `skills/executor/SKILL.md` for single-task implementation
2. Add `executor` scope/checkpoint to `commit-conventions.json`
3. Update routing to handle executor tasks
4. The executor works sequentially through the plan (wave by wave, task by task)
5. Write tests for executor routing

### Phase 3: Agent Identity System

**Goal:** Add agent identity infrastructure without requiring swarm mode.

1. Create `swarm-join.py` for identity generation
2. Add `--agent-id` support to `git-checkpoint.py` for commit trailers
3. Create `.cadence/agents/` directory structure
4. Add `swarm` configuration block to cadence.json schema
5. Write tests for identity generation and trailer formatting

### Phase 4: Parallel-Aware Routing

**Goal:** The workflow state engine understands waves and can offer multiple tasks.

1. Add `next_items` (plural) to `_derive_workflow` output
2. Add `active_wave` computation
3. Implement wave completion detection
4. Add `get_available_tasks` for querying unclaimed work
5. Maintain backward compatibility with `next_item` (singular)
6. Write tests for parallel routing logic

### Phase 5: GitHub Issue Integration

**Goal:** Tasks become GitHub Issues for coordination.

1. Create `create-task-issues.py` for batch issue creation
2. Create `claim-task.py` for atomic issue claiming
3. Create `reconcile-wave-status.py` for reading issue/PR state
4. Create `advance-wave.py` for wave completion and advancement
5. Create `validate-pr-zones.py` for PR validation
6. Write tests with mocked GitHub API responses

### Phase 6: Swarm Skills

**Goal:** The coordinator and worker skills that tie everything together.

1. Create `skills/swarm-coordinator/SKILL.md`
2. Create `skills/swarm-worker/SKILL.md`
3. Add `swarm-coordinator` scope/checkpoint to `commit-conventions.json`
4. Update `SKILL.md` orchestrator with swarm routing
5. Update `project-progress` skill to show swarm status
6. End-to-end testing with mock agents

### Phase 7: Hardening

**Goal:** Recovery mechanisms and edge case handling.

1. Implement stale claim detection and reassignment
2. Implement PR zone validation (can be a GitHub Action)
3. Add git lockfile retry logic (exponential backoff) to all git-calling scripts
4. Add GitHub API rate-limit awareness (`X-RateLimit-Remaining` check, `Retry-After` backoff) to all `gh`-calling scripts
5. Implement double-claim detection in `reconcile-wave-status.py`
6. Add configurable timeouts for stale claims and API retries
7. Implement plan adjustment workflow
8. Add observability (swarm status dashboard via `swarm-status.py`)
9. Stress testing with concurrent simulated agents on the same machine (worktree contention, lockfile races, rate limits)

---

## 14. Configuration

### 14.1 Swarm Configuration in `cadence.json`

```json
{
  "swarm": {
    "branch_prefix": "cadence",
    "issue_label_namespace": "cadence",
    "coordinator_agent_id": "",
    "max_concurrent_agents": 0,
    "wave_advancement": "auto",
    "stale_claim_timeout_hours": 24,
    "merge_strategy": "squash",
    "require_zone_validation": true,
    "git_retry_attempts": 3,
    "git_retry_backoff_ms": 500
  }
}
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `branch_prefix` | string | `"cadence"` | Prefix for agent branches |
| `issue_label_namespace` | string | `"cadence"` | Prefix for GitHub Issue labels |
| `coordinator_agent_id` | string | `""` | Agent ID of the current coordinator |
| `max_concurrent_agents` | int | `0` | Max workers (0 = unlimited) |
| `wave_advancement` | string | `"auto"` | `"auto"` (advance when wave complete) or `"manual"` (coordinator must advance) |
| `stale_claim_timeout_hours` | int | `24` | Hours before a claimed task is considered stale |
| `merge_strategy` | string | `"squash"` | `"squash"` or `"merge"` for PR merging |
| `require_zone_validation` | bool | `true` | Whether PR zone validation is enforced |
| `git_retry_attempts` | int | `3` | Max retries on git lockfile contention |
| `git_retry_backoff_ms` | int | `500` | Initial backoff delay (doubles per retry) for git lockfile retries |

---

## 15. Testing Strategy

### 15.1 Unit Tests (Python)

| Test File | Coverage |
|-----------|----------|
| `test_plan_validation.py` | Ownership zone overlap detection, plan structure validation |
| `test_parallel_routing.py` | `next_items` computation, wave completion, task availability |
| `test_agent_identity.py` | ID generation, trailer formatting, agent state file schema |
| `test_issue_creation.py` | Issue body generation, label assignment, metadata embedding |
| `test_claim_task.py` | Branch-first claim protocol, worktree lock, label-swap rollback, double-claim detection |
| `test_git_retry.py` | Lockfile contention retry logic, exponential backoff, max-attempt exhaustion |
| `test_wave_advancement.py` | Wave completion detection, next-wave computation |
| `test_zone_validation.py` | PR diff vs ownership zone checking |
| `test_swarm_backward_compat.py` | Single-agent mode unaffected by swarm code paths |

### 15.2 Integration Tests

- Simulated multi-agent scenario with mock GitHub API
- Verify: issue creation → claim → branch → PR → merge → wave advance
- Verify: ownership zone violations detected and blocked
- Verify: stale claim recovery works
- Verify: coordinator restart reconstructs state
- Verify: same-machine claim race -- two agents claiming the same task, second agent's worktree add fails and it moves on
- Verify: cross-machine claim race -- two agents both succeed at worktree but one loses the label check and rolls back
- Verify: git lockfile retry -- simulate lockfile contention and confirm retry succeeds within configured attempts
- Verify: GitHub API rate limit -- mock 403 response, confirm agent pauses and retries with backoff

### 15.3 Compatibility Tests

- All existing tests must continue passing
- Single-agent workflow must be unaffected
- New cadence.json fields must have safe defaults
- `reconcile_workflow_state` must handle pre-swarm state files

---

## 16. Open Questions and Future Considerations

### 16.1 Decided

- **Agent identity mechanism:** Commit trailers + branch names (not GitHub accounts)
- **Coordination layer:** GitHub Issues + PRs (not file-based distributed state)
- **Parallelism boundary:** Waves (not arbitrary task-level dependencies)
- **Conflict prevention:** Ownership zones declared at plan time
- **State contention:** Coordinator-only writes to main cadence.json

### 16.2 Open Questions

1. **Should the coordinator be an AI agent or always a human?** The plan supports both, but an AI coordinator introduces a recursive orchestration challenge (who coordinates the coordinator?). Start with human coordinator + AI workers, add AI coordinator later.

2. **How should agents handle tasks that need context from other agents' work?** The wave boundary handles this (Wave N+1 starts after Wave N merges), but what about mid-wave context sharing? Probably not needed if ownership zones are well-defined.

3. **Should there be a GitHub Action for automated zone validation?** This would catch violations without requiring coordinator intervention. Worth implementing in Phase 7 but not blocking.

4. **How should `ideation-updater` interact with swarm mode?** If ideation changes mid-execution, the plan may need replanning. This should pause the swarm, replan, and resume. Details TBD.

5. **What happens when the project has a monorepo structure?** Ownership zones need to account for package/workspace boundaries. The planner should detect monorepo signals (from brownfield baseline) and adjust zone granularity.

### 16.3 Future Enhancements (Not In Scope)

- **Agent specialization:** Different agents have different capabilities (e.g., frontend specialist, database specialist). The coordinator assigns tasks based on capability matching.
- **Dynamic agent scaling:** Automatically spawn/terminate agent sessions based on available work.
- **Cross-repository swarms:** Agents working across multiple repos in a monorepo or multi-repo setup.
- **Conflict resolution AI:** An agent that automatically resolves merge conflicts when ownership zones are insufficient.
- **Swarm analytics:** Dashboard showing agent productivity, task completion rates, conflict frequency.

---

## 17. Summary

Swarm execution is Cadence's first-class model for project delivery. A single agent is a swarm of one -- the same coordination primitives apply whether one agent or ten are working.

1. **Adding a planner skill** that produces multi-milestone roadmaps with parallel-safe task decomposition
2. **Introducing agent identity** via commit trailers and branch names (no GitHub accounts needed)
3. **Using GitHub Issues and PRs** as the distributed coordination layer (no state file contention)
4. **Defining ownership zones** that prevent merge conflicts between parallel agents
5. **Splitting roles** into coordinator (manages plan/queue) and workers (implement tasks)
6. **Isolating agents via git worktrees** so multiple agents on the same machine work without interference

The system is designed to be GitHub-native, account-agnostic, and recoverable from any failure mode by reconstructing state from GitHub's durable primitives.
